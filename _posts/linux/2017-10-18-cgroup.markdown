---
layout: post
title:  "Introduce to cgroup"
date:   2017-10-18 21:16:01 +0800
categories: linux
---

***cgroup introduction***




## what is cgroup

cgroups (abbreviated from control groups) is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes.

Engineers at Google (primarily Paul Menage and Rohit Seth) started the work on this feature in 2006 under the name "process containers". In late 2007, the nomenclature changed to "control groups" to avoid confusion caused by multiple meanings of the term "container" in the Linux kernel context, and the control groups functionality was merged into the Linux kernel mainline in kernel version 2.6.24, which was released in January 2008. Since then, developers have added many new features and controllers, such as support for kernfs, firewalling, and unified hierarchy.


## what in cgroup
run command on linux:

```
$ ls /sys/fs/cgroup/ -al
total 0
drwxr-xr-x 13 root root 340 Oct 19 18:05 .
drwxr-xr-x  9 root root   0 Oct 20  2017 ..
dr-xr-xr-x  5 root root   0 Oct 19 18:05 blkio
lrwxrwxrwx  1 root root  11 Oct 19 18:05 cpu -> cpu,cpuacct
lrwxrwxrwx  1 root root  11 Oct 19 18:05 cpuacct -> cpu,cpuacct
dr-xr-xr-x  7 root root   0 Oct 19 18:45 cpu,cpuacct
dr-xr-xr-x  2 root root   0 Oct 19 18:05 cpuset
dr-xr-xr-x  5 root root   0 Oct 19 18:05 devices
dr-xr-xr-x  2 root root   0 Oct 19 18:05 freezer
dr-xr-xr-x  2 root root   0 Oct 19 18:05 hugetlb
dr-xr-xr-x  5 root root   0 Oct 19 18:05 memory
lrwxrwxrwx  1 root root  16 Oct 19 18:05 net_cls -> net_cls,net_prio
dr-xr-xr-x  2 root root   0 Oct 19 18:05 net_cls,net_prio
lrwxrwxrwx  1 root root  16 Oct 19 18:05 net_prio -> net_cls,net_prio
dr-xr-xr-x  2 root root   0 Oct 19 18:05 perf_event
dr-xr-xr-x  5 root root   0 Oct 19 18:05 pids
dr-xr-xr-x  5 root root   0 Oct 19 18:05 systemd
```


### blkio
```
$ ls /sys/fs/cgroup/blkio/ -al
total 0
dr-xr-xr-x  5 root root   0 Oct 19 21:10 .
drwxr-xr-x 13 root root 340 Oct 19 18:05 ..
-r--r--r--  1 root root   0 Oct 19 21:13 blkio.io_merged
-r--r--r--  1 root root   0 Oct 19 21:13 blkio.io_merged_recursive
-r--r--r--  1 root root   0 Oct 19 21:13 blkio.io_queued
-r--r--r--  1 root root   0 Oct 19 21:13 blkio.io_queued_recursive
-r--r--r--  1 root root   0 Oct 19 21:13 blkio.io_service_bytes
-r--r--r--  1 root root   0 Oct 19 21:13 blkio.io_service_bytes_recursive
-r--r--r--  1 root root   0 Oct 19 21:13 blkio.io_serviced
-r--r--r--  1 root root   0 Oct 19 21:13 blkio.io_serviced_recursive
-r--r--r--  1 root root   0 Oct 19 21:13 blkio.io_service_time
-r--r--r--  1 root root   0 Oct 19 21:13 blkio.io_service_time_recursive
-r--r--r--  1 root root   0 Oct 19 21:13 blkio.io_wait_time
-r--r--r--  1 root root   0 Oct 19 21:13 blkio.io_wait_time_recursive
-rw-r--r--  1 root root   0 Oct 19 21:13 blkio.leaf_weight
-rw-r--r--  1 root root   0 Oct 19 21:13 blkio.leaf_weight_device
--w-------  1 root root   0 Oct 19 21:13 blkio.reset_stats
-r--r--r--  1 root root   0 Oct 19 21:13 blkio.sectors
-r--r--r--  1 root root   0 Oct 19 21:13 blkio.sectors_recursive
-r--r--r--  1 root root   0 Oct 19 21:13 blkio.throttle.io_service_bytes
-r--r--r--  1 root root   0 Oct 19 21:13 blkio.throttle.io_serviced
-rw-r--r--  1 root root   0 Oct 19 18:05 blkio.throttle.read_bps_device
-rw-r--r--  1 root root   0 Oct 19 18:05 blkio.throttle.read_iops_device
-rw-r--r--  1 root root   0 Oct 19 18:05 blkio.throttle.write_bps_device
-rw-r--r--  1 root root   0 Oct 19 18:05 blkio.throttle.write_iops_device
-r--r--r--  1 root root   0 Oct 19 21:13 blkio.time
-r--r--r--  1 root root   0 Oct 19 21:13 blkio.time_recursive
-rw-r--r--  1 root root   0 Oct 19 18:05 blkio.weight
-rw-r--r--  1 root root   0 Oct 19 18:05 blkio.weight_device
-rw-r--r--  1 root root   0 Oct 19 21:13 cgroup.clone_children
-rw-r--r--  1 root root   0 Oct 19 18:30 cgroup.procs
-r--r--r--  1 root root   0 Oct 19 21:13 cgroup.sane_behavior
drwxr-xr-x  2 root root   0 Oct 19 18:05 init.scope
-rw-r--r--  1 root root   0 Oct 19 21:13 notify_on_release
-rw-r--r--  1 root root   0 Oct 19 21:13 release_agent
drwxr-xr-x 64 root root   0 Oct 19 18:05 system.slice
-rw-r--r--  1 root root   0 Oct 19 21:13 tasks
drwxr-xr-x  2 root root   0 Oct 19 18:05 user.slice
```
cgroup subsys "blkio" implements the block io controller. There seems to be
a need of various kinds of IO control policies (like proportional BW, max BW)
both at leaf nodes as well as at intermediate nodes in a storage hierarchy.
Plan is to use the same cgroup based management interface for blkio controller
and based on user options switch IO policies in the background.

Currently two IO control policies are implemented. First one is proportional
weight time based division of disk policy. It is implemented in CFQ. Hence
this policy takes effect only on leaf nodes when CFQ is being used. The second
one is throttling policy which can be used to specify upper IO rate limits
on devices. This policy is implemented in generic block layer and can be
used on leaf nodes as well as higher level logical devices like device mapper.


### cpu,cpuacct
```
$ ls /sys/fs/cgroup/cpu,cpuacct/ -al
total 0
dr-xr-xr-x  7 root root   0 Oct 19 18:45 .
drwxr-xr-x 13 root root 340 Oct 19 18:05 ..
-rw-r--r--  1 root root   0 Oct 19 18:50 cgroup.clone_children
-rw-r--r--  1 root root   0 Oct 19 18:50 cgroup.procs
-r--r--r--  1 root root   0 Oct 19 18:50 cgroup.sane_behavior
-r--r--r--  1 root root   0 Oct 19 18:50 cpuacct.stat
-rw-r--r--  1 root root   0 Oct 19 18:50 cpuacct.usage
-r--r--r--  1 root root   0 Oct 19 18:50 cpuacct.usage_percpu
-rw-r--r--  1 root root   0 Oct 19 18:50 cpu.cfs_period_us
-rw-r--r--  1 root root   0 Oct 19 18:50 cpu.cfs_quota_us
-rw-r--r--  1 root root   0 Oct 19 18:50 cpu.shares
-r--r--r--  1 root root   0 Oct 19 18:50 cpu.stat
drwxr-xr-x  2 root root   0 Oct 19 18:50 init.scope
-rw-r--r--  1 root root   0 Oct 19 18:50 notify_on_release
-rw-r--r--  1 root root   0 Oct 19 18:50 release_agent
drwxr-xr-x 64 root root   0 Oct 19 18:50 system.slice
-rw-r--r--  1 root root   0 Oct 19 18:50 tasks
drwxr-xr-x  2 root root   0 Oct 19 18:50 user.slice
```
The CPU accounting controller is used to group tasks using cgroups and
account the CPU usage of these groups of tasks.

The CPU accounting controller supports multi-hierarchy groups. An accounting
group accumulates the CPU usage of all of its child groups and the tasks
directly present in its group.


### cpuset
```
$ ls /sys/fs/cgroup/cpuset/ -al
total 0
dr-xr-xr-x  2 root root   0 Oct 19 21:10 .
drwxr-xr-x 13 root root 340 Oct 19 18:05 ..
-rw-r--r--  1 root root   0 Oct 19 21:14 cgroup.clone_children
-rw-r--r--  1 root root   0 Oct 19 21:14 cgroup.procs
-r--r--r--  1 root root   0 Oct 19 21:14 cgroup.sane_behavior
-rw-r--r--  1 root root   0 Oct 19 21:14 cpuset.cpu_exclusive
-rw-r--r--  1 root root   0 Oct 19 18:05 cpuset.cpus
-r--r--r--  1 root root   0 Oct 19 21:14 cpuset.effective_cpus
-r--r--r--  1 root root   0 Oct 19 21:14 cpuset.effective_mems
-rw-r--r--  1 root root   0 Oct 19 21:14 cpuset.mem_exclusive
-rw-r--r--  1 root root   0 Oct 19 21:14 cpuset.mem_hardwall
-rw-r--r--  1 root root   0 Oct 19 21:14 cpuset.memory_migrate
-r--r--r--  1 root root   0 Oct 19 21:14 cpuset.memory_pressure
-rw-r--r--  1 root root   0 Oct 19 21:14 cpuset.memory_pressure_enabled
-rw-r--r--  1 root root   0 Oct 19 21:14 cpuset.memory_spread_page
-rw-r--r--  1 root root   0 Oct 19 21:14 cpuset.memory_spread_slab
-rw-r--r--  1 root root   0 Oct 19 18:05 cpuset.mems
-rw-r--r--  1 root root   0 Oct 19 21:14 cpuset.sched_load_balance
-rw-r--r--  1 root root   0 Oct 19 21:14 cpuset.sched_relax_domain_level
-rw-r--r--  1 root root   0 Oct 19 21:14 notify_on_release
-rw-r--r--  1 root root   0 Oct 19 21:14 release_agent
-rw-r--r--  1 root root   0 Oct 19 21:14 tasks
```
Cpusets provide a mechanism for assigning a set of CPUs and Memory
Nodes to a set of tasks.   In this document "Memory Node" refers to
an on-line node that contains memory.

Cpusets constrain the CPU and Memory placement of tasks to only
the resources within a task's current cpuset.  They form a nested
hierarchy visible in a virtual file system.  These are the essential
hooks, beyond what is already present, required to manage dynamic
job placement on large systems.


### devices
```
$ ls /sys/fs/cgroup/devices/ -al
total 0
dr-xr-xr-x  5 root root   0 Oct 19 21:10 .
drwxr-xr-x 13 root root 340 Oct 19 18:05 ..
-rw-r--r--  1 root root   0 Oct 19 21:15 cgroup.clone_children
-rw-r--r--  1 root root   0 Oct 19 18:30 cgroup.procs
-r--r--r--  1 root root   0 Oct 19 21:15 cgroup.sane_behavior
--w-------  1 root root   0 Oct 19 21:15 devices.allow
--w-------  1 root root   0 Oct 19 21:15 devices.deny
-r--r--r--  1 root root   0 Oct 19 21:15 devices.list
drwxr-xr-x  2 root root   0 Oct 19 18:05 init.scope
-rw-r--r--  1 root root   0 Oct 19 21:15 notify_on_release
-rw-r--r--  1 root root   0 Oct 19 21:15 release_agent
drwxr-xr-x 64 root root   0 Oct 19 18:05 system.slice
-rw-r--r--  1 root root   0 Oct 19 21:15 tasks
drwxr-xr-x  2 root root   0 Oct 19 18:05 user.slice
```
Implement a cgroup to track and enforce open and mknod restrictions
on device files.  A device cgroup associates a device access
whitelist with each cgroup.  A whitelist entry has 4 fields.
'type' is a (all), c (char), or b (block).  'all' means it applies
to all types and all major and minor numbers.  Major and minor are
either an integer or * for all.  Access is a composition of r
(read), w (write), and m (mknod).

The root device cgroup starts with rwm to 'all'.  A child device
cgroup gets a copy of the parent.  Administrators can then remove
devices from the whitelist or add new entries.  A child cgroup can
never receive a device access which is denied by its parent.


### freezer
```
$ ls /sys/fs/cgroup/freezer/ -al
total 0
dr-xr-xr-x  2 root root   0 Oct 19 21:10 .
drwxr-xr-x 13 root root 340 Oct 19 18:05 ..
-rw-r--r--  1 root root   0 Oct 19 21:16 cgroup.clone_children
-rw-r--r--  1 root root   0 Oct 19 21:16 cgroup.procs
-r--r--r--  1 root root   0 Oct 19 21:16 cgroup.sane_behavior
-rw-r--r--  1 root root   0 Oct 19 21:16 notify_on_release
-rw-r--r--  1 root root   0 Oct 19 21:16 release_agent
-rw-r--r--  1 root root   0 Oct 19 21:16 tasks
```
The cgroup freezer is useful to batch job management system which start
and stop sets of tasks in order to schedule the resources of a machine
according to the desires of a system administrator. This sort of program
is often used on HPC clusters to schedule access to the cluster as a
whole. The cgroup freezer uses cgroups to describe the set of tasks to
be started/stopped by the batch job management system. It also provides
a means to start and stop the tasks composing the job.

The cgroup freezer will also be useful for checkpointing running groups
of tasks. The freezer allows the checkpoint code to obtain a consistent
image of the tasks by attempting to force the tasks in a cgroup into a
quiescent state. Once the tasks are quiescent another task can
walk /proc or invoke a kernel interface to gather information about the
quiesced tasks. Checkpointed tasks can be restarted later should a
recoverable error occur. This also allows the checkpointed tasks to be
migrated between nodes in a cluster by copying the gathered information
to another node and restarting the tasks there.


### hugetlb
```
$ ls /sys/fs/cgroup/hugetlb/ -al
total 0
dr-xr-xr-x  2 root root   0 Oct 19 21:10 .
drwxr-xr-x 13 root root 340 Oct 19 18:05 ..
-rw-r--r--  1 root root   0 Oct 19 21:16 cgroup.clone_children
-rw-r--r--  1 root root   0 Oct 19 21:16 cgroup.procs
-r--r--r--  1 root root   0 Oct 19 21:16 cgroup.sane_behavior
-rw-r--r--  1 root root   0 Oct 19 21:16 hugetlb.1GB.failcnt
-rw-r--r--  1 root root   0 Oct 19 21:16 hugetlb.1GB.limit_in_bytes
-rw-r--r--  1 root root   0 Oct 19 21:16 hugetlb.1GB.max_usage_in_bytes
-r--r--r--  1 root root   0 Oct 19 21:16 hugetlb.1GB.usage_in_bytes
-rw-r--r--  1 root root   0 Oct 19 21:16 hugetlb.2MB.failcnt
-rw-r--r--  1 root root   0 Oct 19 21:16 hugetlb.2MB.limit_in_bytes
-rw-r--r--  1 root root   0 Oct 19 21:16 hugetlb.2MB.max_usage_in_bytes
-r--r--r--  1 root root   0 Oct 19 21:16 hugetlb.2MB.usage_in_bytes
-rw-r--r--  1 root root   0 Oct 19 21:16 notify_on_release
-rw-r--r--  1 root root   0 Oct 19 21:16 release_agent
-rw-r--r--  1 root root   0 Oct 19 21:16 tasks
```
The HugeTLB controller allows to limit the HugeTLB usage per control group and
enforces the controller limit during page fault. Since HugeTLB doesn't
support page reclaim, enforcing the limit at page fault time implies that,
the application will get SIGBUS signal if it tries to access HugeTLB pages
beyond its limit. This requires the application to know beforehand how much
HugeTLB pages it would require for its use.


### memeory
```
$ ls /sys/fs/cgroup/memory/ -al
total 0
dr-xr-xr-x  5 root root   0 Oct 19 21:10 .
drwxr-xr-x 13 root root 340 Oct 19 18:05 ..
-rw-r--r--  1 root root   0 Oct 19 21:17 cgroup.clone_children
--w--w--w-  1 root root   0 Oct 19 21:17 cgroup.event_control
-rw-r--r--  1 root root   0 Oct 19 18:30 cgroup.procs
-r--r--r--  1 root root   0 Oct 19 21:17 cgroup.sane_behavior
drwxr-xr-x  2 root root   0 Oct 19 18:05 init.scope
-rw-r--r--  1 root root   0 Oct 19 21:17 memory.failcnt
--w-------  1 root root   0 Oct 19 21:17 memory.force_empty
-rw-r--r--  1 root root   0 Oct 19 21:17 memory.kmem.failcnt
-rw-r--r--  1 root root   0 Oct 19 18:05 memory.kmem.limit_in_bytes
-rw-r--r--  1 root root   0 Oct 19 21:17 memory.kmem.max_usage_in_bytes
-r--r--r--  1 root root   0 Oct 19 21:17 memory.kmem.slabinfo
-rw-r--r--  1 root root   0 Oct 19 21:17 memory.kmem.tcp.failcnt
-rw-r--r--  1 root root   0 Oct 19 21:17 memory.kmem.tcp.limit_in_bytes
-rw-r--r--  1 root root   0 Oct 19 21:17 memory.kmem.tcp.max_usage_in_bytes
-r--r--r--  1 root root   0 Oct 19 21:17 memory.kmem.tcp.usage_in_bytes
-r--r--r--  1 root root   0 Oct 19 21:17 memory.kmem.usage_in_bytes
-rw-r--r--  1 root root   0 Oct 19 21:17 memory.limit_in_bytes
-rw-r--r--  1 root root   0 Oct 19 21:17 memory.max_usage_in_bytes
-rw-r--r--  1 root root   0 Oct 19 21:17 memory.move_charge_at_immigrate
-r--r--r--  1 root root   0 Oct 19 21:17 memory.numa_stat
-rw-r--r--  1 root root   0 Oct 19 18:05 memory.oom_control
----------  1 root root   0 Oct 19 21:17 memory.pressure_level
-rw-r--r--  1 root root   0 Oct 19 18:05 memory.soft_limit_in_bytes
-r--r--r--  1 root root   0 Oct 19 21:17 memory.stat
-rw-r--r--  1 root root   0 Oct 19 18:05 memory.swappiness
-r--r--r--  1 root root   0 Oct 19 21:17 memory.usage_in_bytes
-rw-r--r--  1 root root   0 Oct 19 18:05 memory.use_hierarchy
-rw-r--r--  1 root root   0 Oct 19 21:17 notify_on_release
-rw-r--r--  1 root root   0 Oct 19 21:17 release_agent
drwxr-xr-x 64 root root   0 Oct 19 18:05 system.slice
-rw-r--r--  1 root root   0 Oct 19 21:17 tasks
drwxr-xr-x  2 root root   0 Oct 19 18:05 user.slice
```
The memory controller isolates the memory behaviour of a group of tasks
from the rest of the system. The article on LWN [12] mentions some probable
uses of the memory controller. The memory controller can be used to

a. Isolate an application or a group of applications
   Memory-hungry applications can be isolated and limited to a smaller
   amount of memory.
b. Create a cgroup with a limited amount of memory; this can be used
   as a good alternative to booting with mem=XXXX.
c. Virtualization solutions can control the amount of memory they want
   to assign to a virtual machine instance.
d. A CD/DVD burner could control the amount of memory used by the
   rest of the system to ensure that burning does not fail due to lack
   of available memory.
e. There are several other use cases; find one or use the controller just
   for fun (to learn and hack on the VM subsystem).


### net_cls,net_prio
```
$ ls /sys/fs/cgroup/net_cls,net_prio/ -al
total 0
dr-xr-xr-x  2 root root   0 Oct 19 21:10 .
drwxr-xr-x 13 root root 340 Oct 19 18:05 ..
-rw-r--r--  1 root root   0 Oct 19 21:18 cgroup.clone_children
-rw-r--r--  1 root root   0 Oct 19 21:18 cgroup.procs
-r--r--r--  1 root root   0 Oct 19 21:18 cgroup.sane_behavior
-rw-r--r--  1 root root   0 Oct 19 21:18 net_cls.classid
-rw-r--r--  1 root root   0 Oct 19 21:18 net_prio.ifpriomap
-r--r--r--  1 root root   0 Oct 19 21:18 net_prio.prioidx
-rw-r--r--  1 root root   0 Oct 19 21:18 notify_on_release
-rw-r--r--  1 root root   0 Oct 19 21:18 release_agent
-rw-r--r--  1 root root   0 Oct 19 21:18 tasks
```
The Network classifier cgroup provides an interface to
tag network packets with a class identifier (classid).

The Traffic Controller (tc) can be used to assign
different priorities to packets from different cgroups.
Also, Netfilter (iptables) can use this tag to perform
actions on such packets.

Creating a net_cls cgroups instance creates a net_cls.classid file.
This net_cls.classid value is initialized to 0.

You can write hexadecimal values to net_cls.classid; the format for these
values is 0xAAAABBBB; AAAA is the major handle number and BBBB
is the minor handle number.
Reading net_cls.classid yields a decimal result.

The Network priority cgroup provides an interface to allow an administrator to
dynamically set the priority of network traffic generated by various
applications

Nominally, an application would set the priority of its traffic via the
SO_PRIORITY socket option.  This however, is not always possible because:

1) The application may not have been coded to set this value
2) The priority of application traffic is often a site-specific administrative
   decision rather than an application defined one.


### perf_event
```
$ ls /sys/fs/cgroup/perf_event/ -al
total 0
dr-xr-xr-x  2 root root   0 Oct 19 21:10 .
drwxr-xr-x 13 root root 340 Oct 19 18:05 ..
-rw-r--r--  1 root root   0 Oct 19 21:19 cgroup.clone_children
-rw-r--r--  1 root root   0 Oct 19 21:19 cgroup.procs
-r--r--r--  1 root root   0 Oct 19 21:19 cgroup.sane_behavior
-rw-r--r--  1 root root   0 Oct 19 21:19 notify_on_release
-rw-r--r--  1 root root   0 Oct 19 21:19 release_agent
-rw-r--r--  1 root root   0 Oct 19 21:19 tasks
```
perf_event controller, if not mounted on a legacy hierarchy, is
automatically enabled on the v2 hierarchy so that perf events can
always be filtered by cgroup v2 path.  The controller can still be
moved to a legacy hierarchy after v2 hierarchy is populated.


### pids
```
$ ls /sys/fs/cgroup/pids/ -al
total 0
dr-xr-xr-x  5 root root   0 Oct 19 21:10 .
drwxr-xr-x 13 root root 340 Oct 19 18:05 ..
-rw-r--r--  1 root root   0 Oct 19 21:19 cgroup.clone_children
-rw-r--r--  1 root root   0 Oct 19 18:30 cgroup.procs
-r--r--r--  1 root root   0 Oct 19 21:19 cgroup.sane_behavior
drwxr-xr-x  2 root root   0 Oct 19 18:05 init.scope
-rw-r--r--  1 root root   0 Oct 19 21:19 notify_on_release
-rw-r--r--  1 root root   0 Oct 19 21:19 release_agent
drwxr-xr-x 64 root root   0 Oct 19 18:05 system.slice
-rw-r--r--  1 root root   0 Oct 19 21:19 tasks
drwxr-xr-x  3 root root   0 Oct 19 18:05 user.slice
```
The process number controller is used to allow a cgroup hierarchy to stop any
new tasks from being fork()'d or clone()'d after a certain limit is reached.

Since it is trivial to hit the task limit without hitting any kmemcg limits in
place, PIDs are a fundamental resource. As such, PID exhaustion must be
preventable in the scope of a cgroup hierarchy by allowing resource limiting of
the number of tasks in a cgroup.


### systemd
```
$ ls /sys/fs/cgroup/systemd/ -al
total 0
dr-xr-xr-x  5 root root   0 Oct 19 21:10 .
drwxr-xr-x 13 root root 340 Oct 19 18:05 ..
-rw-r--r--  1 root root   0 Oct 19 21:20 cgroup.clone_children
-rw-r--r--  1 root root   0 Oct 19 18:05 cgroup.procs
-r--r--r--  1 root root   0 Oct 19 21:20 cgroup.sane_behavior
drwxr-xr-x  2 root root   0 Oct 19 18:05 init.scope
-rw-r--r--  1 root root   0 Oct 19 18:05 notify_on_release
-rw-r--r--  1 root root   0 Oct 19 18:05 release_agent
drwxr-xr-x 64 root root   0 Oct 19 18:05 system.slice
-rw-r--r--  1 root root   0 Oct 19 21:20 tasks
drwxr-xr-x  3 root root   0 Oct 19 18:05 user.slice
```